WEBVTT
Kind: captions
Language: en-GB

00:00:03.360 --> 00:00:08.130
Peter: Whenever I give a talk somewhere to
a general audience about robots, somebody

00:00:08.130 --> 00:00:13.519
always asks me the question about jobs; won’t
robots take our jobs away? And the argument

00:00:13.519 --> 00:00:19.780
I usually make is 'yeah, they're going to
take some jobs away and that's uncomfortable,

00:00:19.780 --> 00:00:24.430
but the alternative is if we don't have robots
working in factories, the likelihood is we

00:00:24.430 --> 00:00:27.080
will lose the industry entirely.'

00:00:27.080 --> 00:00:30.750
So we're making a bit of a trade. We're going
to keep the industry, for instance, we're

00:00:30.750 --> 00:00:36.160
going to keep manufacturing automobiles in
a high labor cost country and it's going to

00:00:36.160 --> 00:00:40.590
employ some people and it's going to employ
some taxes and there's going to be a big knock

00:00:40.590 --> 00:00:46.500
on economic benefit, but yeah, absolutely,
it will have some impact on jobs.

00:00:46.500 --> 00:00:51.399
More recently, there's been quite a bit written
about the so-called new machine age where

00:00:51.399 --> 00:00:58.200
a whole raft the jobs, not necessarily manual
labor jobs, a lot of so-called knowledge worker

00:00:58.200 --> 00:01:04.110
jobs will perhaps disappear, like interpreting
X-rays and writing financial reports. Now

00:01:04.110 --> 00:01:06.509
these are jobs now that are on the line.

00:01:06.509 --> 00:01:13.000
There's a once upon a time very famous philosopher,
Voltaire, talked about the virtues of work

00:01:13.000 --> 00:01:19.350
and many of us define ourselves or our identity,
our self-esteem, it’s all wrapped up in

00:01:19.350 --> 00:01:25.570
work. So this seems to me perhaps the most exquisitely complex of all the topics that we've talked about today.

00:01:25.570 --> 00:01:28.330
What does ethics say about this?

00:01:29.240 --> 00:01:40.260
Doug: Our identity is locked up in what we
do often for most of us. And our fundamental

00:01:40.270 --> 00:01:46.369
moral philosophy that guides most of us comes
out of John Locke's theory of labor. John

00:01:46.369 --> 00:01:54.350
Locke's theory of labor is the foundation
to the rights to property. And that in itself

00:01:54.350 --> 00:01:59.420
set up the property rights basis of the American
Constitution.

00:01:59.420 --> 00:02:05.780
So the rights to property and how we define
ourselves and how we define our culture is

00:02:05.780 --> 00:02:14.500
locked up in what we do. And it's entrenched
in terms of liberal democracy, the labor theory,

00:02:14.500 --> 00:02:22.620
and utilitarianism; all three of those philosophical perspectives guide us in terms of how we set our values.

00:02:22.620 --> 00:02:32.080
Now with respect to machines replacing humans,
you'll be very familiar with the Luddite revolution.

00:02:32.080 --> 00:02:32.980
Peter: Absolutely.

00:02:32.980 --> 00:02:34.930
Doug: During the early industrial revolution.

00:02:34.930 --> 00:02:36.300
Peter: The guys with big hammers.

00:02:36.300 --> 00:02:40.660
Doug: Yep. They went in and they smashed all
the cotton mills. I think it's about 1790.

00:02:40.660 --> 00:02:44.209
They went in there and just went to town on
the cotton mills because they're afraid, and

00:02:44.209 --> 00:02:48.769
they were quite right in this, that the cotton
mills are going to replace their jobs. And they did.

00:02:48.770 --> 00:02:56.870
Of course, the Luddites were all duly hanged
very quickly. Victorian rules for misconduct

00:02:56.870 --> 00:03:02.019
were some of the most severe in the history
in the world but thank heavens we don't have

00:03:02.019 --> 00:03:06.440
so many hard laws anymore.

00:03:06.440 --> 00:03:13.450
But the Luddites had a justified position.
However, that's been with us since machines

00:03:13.450 --> 00:03:19.760
been with us, that fear. That innate fear
that we're going to lose not only our jobs,

00:03:19.760 --> 00:03:23.660
but our identity and who we are.

00:03:23.660 --> 00:03:29.209
Peter: And since industrialisation here, so
many jobs have been created and then disappeared.

00:03:29.209 --> 00:03:35.140
Doug: Correct. It seems to ebb and flow. We're
getting more and more people in the labor

00:03:35.140 --> 00:03:40.580
market. Australia in the last 50 years has
doubled in population. We have more people working.

00:03:42.120 --> 00:03:47.060
So I might be a little ignorant on this, but
from what I can see is that we adapt. It's

00:03:47.060 --> 00:03:53.010
one of the most interesting things about the
human being, is our ability to adapt and to

00:03:53.010 --> 00:03:56.209
change. And we seem to be doing that.

00:03:56.209 --> 00:04:04.230
And so, this innate fear of losing our jobs
to machines seems to be unfounded. However,

00:04:04.230 --> 00:04:11.799
I understand it. In terms of ethics, really,
it's not an ethical position that you can

00:04:11.799 --> 00:04:17.319
take. I guess individuals need to have the
autonomy to choose and to be able to work

00:04:17.319 --> 00:04:23.360
and to be able to live in a free environment
and to have access to those resources, so

00:04:23.360 --> 00:04:26.689
that's very important ethically.

00:04:26.689 --> 00:04:31.710
But the argument that machines are going to
take that away, I've seen some evidence of

00:04:31.710 --> 00:04:39.169
it recently with the closure of the Holden
facilities in South Australia, but bottom

00:04:39.169 --> 00:04:42.409
line is - now we're talking about submarines,
I think, yesterday on the news?

00:04:42.409 --> 00:04:42.849
Peter: Yep.

00:04:42.849 --> 00:04:47.629
Doug: And integrating maybe more work back
into Australia and send it off, putting it

00:04:47.629 --> 00:04:55.539
into Japan for re-designing and rebuilding
submarines back in Australia. So, it ebbs and flows.

00:04:55.539 --> 00:05:03.110
Peter: So in the case of an individual, if
they don't have a job, the ethical consideration

00:05:03.110 --> 00:05:07.270
here is whether we impinge on their autonomy.

00:05:07.270 --> 00:05:13.639
Doug: Yes, correct. And there are basic right
to have a fulfilled life in this culture.

00:05:13.639 --> 00:05:18.419
Peter: But in our culture a fulfilled life
usually includes a job.

00:05:18.419 --> 00:05:25.979
Doug: Well, it includes the right to work
and the right to have access to work and resources.

00:05:25.979 --> 00:05:26.930
Yes, it does.

00:05:26.930 --> 00:05:33.550
Peter: So we could put a roof over people's
heads, we can make sure that they're not hungry

00:05:33.550 --> 00:05:42.470
and so they would be in some sense looked after, comfortable, but they may not have the option of a job.

00:05:42.720 --> 00:05:51.399
Doug: Yeah. I think the bigger question is, are machines taking away jobs? In other words, if we look at…

00:05:51.399 --> 00:05:52.869
Peter: In an overall sense.

00:05:52.869 --> 00:05:55.520
Peter: Yes, they're going to take away some
but then other jobs are going to come.

00:05:55.529 --> 00:06:00.080
Doug: And other jobs will be created with
the machines, so there seems to be a balance

00:06:00.080 --> 00:06:05.309
there. And as always, there's a balance, and
I think that there's a cohesion in terms of

00:06:05.309 --> 00:06:09.699
the creation of jobs and the loss of jobs
that seems to be working on. I don't have

00:06:09.699 --> 00:06:11.449
stats on that but that would be an interesting…

00:06:11.449 --> 00:06:14.889
Peter: I mean, it's the way it's gone for
hundred years or more.

00:06:14.889 --> 00:06:15.319
Doug: Absolutely, yeah.

00:06:15.319 --> 00:06:20.969
Peter: The issue is whether we're going to
some kind of tipping point where suddenly

00:06:20.969 --> 00:06:27.860
that no longer holds, and there is some books
that say that that's case, but I don't think

00:06:27.860 --> 00:06:32.119
we- I'm not sure anyone can make a hybrid
hard or firm prediction about that.

00:06:32.119 --> 00:06:36.679
Doug: Yeah, so I don't have an answer to that
because if it's the case, let's project…

00:06:36.679 --> 00:06:37.189
Peter: Okay.

00:06:37.189 --> 00:06:48.969
Doug: If it's the case, say, in 100 years,
that machines have really pretty much replaced you and me.

00:06:48.969 --> 00:06:49.469
Peter: Yup.

00:06:49.469 --> 00:06:54.539
Doug: We don't need professors anymore, we've
got an android there that forgot more about

00:06:54.539 --> 00:06:59.649
ethics than Doug Baker will ever know, and
we've got another one over here that, well,

00:06:59.649 --> 00:07:04.860
he's got Peter Corke's brain programmed into
him and he's got about three more other brains

00:07:04.860 --> 00:07:10.419
programmed into him and he can teach everything
there is about robotics.

00:07:10.419 --> 00:07:17.330
So the question then is this, what are guys
like you and me do? We'll probably be doing

00:07:17.330 --> 00:07:27.669
something but I'm not sure what. And the question
then is if we have the jobs taken over by

00:07:27.669 --> 00:07:34.339
machines, what do humans do? Do we get value
for that or maybe we retire at the age of

00:07:34.339 --> 00:07:37.020
30, or maybe we don't work? I don't know.

00:07:37.020 --> 00:07:41.679
Peter: Which is the other thing that I think
is we grow up. We talked about that the people

00:07:41.679 --> 00:07:45.599
would be retiring earlier and all this, and
that didn't happen either.

00:07:45.599 --> 00:07:45.949
Doug: No.

00:07:45.949 --> 00:07:47.369
Peter: The retirement age is going up.

00:07:47.369 --> 00:07:51.309
Doug: Yeah, absolutely. Yeah, absolutely,
it is going up. I don't know what that's about.

00:07:51.309 --> 00:07:54.959
Peter: No, but I think it's a bad idea.

00:07:54.959 --> 00:08:03.779
Doug: That's true. But I think that, in terms
of ethics, the taking away of jobs, I think

00:08:03.779 --> 00:08:07.569
we need to look at the bigger picture and
the balance of what's created and what's taken

00:08:07.569 --> 00:08:13.229
away before we can make that argument. But
it really isn't an argument for ethics. It

00:08:13.229 --> 00:08:19.149
can be an argument for autonomy but overall,
I don't see that's an ethical issue. Not at all.

