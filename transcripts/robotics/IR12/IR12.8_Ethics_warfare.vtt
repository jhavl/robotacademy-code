WEBVTT
Kind: captions
Language: en-GB

00:00:03.610 --> 00:00:07.670
Peter: It's a sad fact that human beings have
a shocking propensity for conflict. It’s

00:00:07.670 --> 00:00:12.910
estimated in the last century, 230 something
million people were killed in war and most

00:00:12.910 --> 00:00:19.199
of them were non-combatants. Many rich countries
now are pushing really hard into robotic technology

00:00:19.199 --> 00:00:24.830
to reduce soldier casualty rates, and developing
so called smart weapons, which will reduce

00:00:24.830 --> 00:00:30.110
the deaths of non-combatants. So, right now,
these robots are not fully autonomous. There

00:00:30.110 --> 00:00:36.340
is a human being responsible for actually
firing the weapon and causing harm or death

00:00:36.340 --> 00:00:41.980
to another human being. But there seems to
be a growing pressure to take that human being

00:00:41.980 --> 00:00:48.210
out of the firing loop, and to actually have
robots making autonomous decisions about whether

00:00:48.210 --> 00:00:54.070
they kill human beings or not. So, what is
ethics have to say about this situation?

00:00:54.070 --> 00:01:05.110
Doug: I think the term moral agency is important
here. What moral agency is, is does that robot

00:01:05.110 --> 00:01:11.140
or that individual have the capacity to make
a decision whether it's right or wrong.

00:01:11.140 --> 00:01:16.700
Peter: Okay but human beings struggle with this. So, how would we expect the machine to be able to do that?

00:01:16.710 --> 00:01:24.950
Doug: Well, probably, they don't. They don't
at this point. So, without moral agency, then

00:01:24.950 --> 00:01:32.799
ethics isn't applied in this context against the machine. So, the machine is neither ethical nor unethical.

00:01:32.799 --> 00:01:34.770
Peter: Because it cannot tell right from wrong?

00:01:34.770 --> 00:01:39.040
Doug: It has no moral agency but what that
means though is that if that's the machine

00:01:39.040 --> 00:01:45.799
without moral agency, then we come back to
the person running or programming the machine.

00:01:45.799 --> 00:01:51.750
They become liable for that moral agency.
So, it just falls back. And so that individual,

00:01:51.750 --> 00:01:57.880
those engineers or that robot designer is
the one that has the ethical responsibility.

00:01:57.880 --> 00:02:02.130
Peter: Okay because of the machine that they
have created, does not have this…

00:02:02.130 --> 00:02:03.590
Doug: There's not have moral agency, correct.

00:02:03.590 --> 00:02:07.860
Peter: …the responsibility is back on the
person who either created it or deployed it.

00:02:07.860 --> 00:02:13.340
Doug: Yes. There's quite a literature I just
discovered on this. They are robot ethics

00:02:13.340 --> 00:02:18.790
and machine ethics… there's quite a bit
out there on this that debates this back and

00:02:18.790 --> 00:02:23.370
forth as you well know. We've talked about
this a little bit. So, for me, that is the

00:02:23.370 --> 00:02:25.690
defining construct. It's moral agency.

00:02:25.690 --> 00:02:31.540
Peter: I know it's a topic that in the robotics
community, we talk a little bit about it.

00:02:31.540 --> 00:02:37.160
I don't think we talked about it nearly enough
but this is the one that really seems to get

00:02:37.160 --> 00:02:44.060
people excited or angry, is this idea of robots
in warfare. It already starting to cause a

00:02:44.060 --> 00:02:44.950
lot of contention.

00:02:44.950 --> 00:02:50.129
Doug: That doesn't surprise me. And Hollywood,
of course, has just done wonders with that,

00:02:50.129 --> 00:02:54.540
with the Arnold Schwarzenegger series, with
the robots taking over the world. It's been

00:02:54.540 --> 00:02:56.080
a common theme for ages.

00:02:56.080 --> 00:03:00.129
Peter: Absolutely. It's the recurring theme
in all robot movies.

00:03:00.129 --> 00:03:07.099
Doug: Yes. Here, we've just discussed beforehand
Asimov, Isaac Asimov and his 1942 short story.

00:03:07.099 --> 00:03:14.209
The short story is called Runaround.

00:03:14.209 --> 00:03:17.849
Peter: Absolutely. This brings in his famous
roles of robotics, yes?

00:03:17.849 --> 00:03:18.660
Doug: Exactly. Shall I read them?

00:03:18.660 --> 00:03:19.260
Peter: Yes.

00:03:19.260 --> 00:03:24.379
I discovered just this recently. So, he's got four laws. A new law was just added.

00:03:24.379 --> 00:03:25.660
Peter: The zeroth law.

00:03:25.660 --> 00:03:30.400
Doug: Yes. According to these, all robots
should understand all circumstances and obey

00:03:30.400 --> 00:03:36.060
these laws.
1. A robot may not injure a human being, or

00:03:36.060 --> 00:03:39.620
through inaction, allow a human being to be
harmed.

00:03:39.620 --> 00:03:46.790
2. A robot must obey orders it receives from
human beings, except when such orders conflict

00:03:46.790 --> 00:03:51.010
with this first law.
Yes, that one's harming.

00:03:51.010 --> 00:03:59.019
3. A robot must protect its own existence
as long as such protection does not conflict

00:03:59.019 --> 00:04:03.319
with the first or second law.
In other words, harming humans. There's another

00:04:03.319 --> 00:04:09.680
one. Asimov added the fourth.
4. No robot may harm humanity, or through

00:04:09.680 --> 00:04:17.650
inaction, allow humanity to come to harm.
So, this sets the basis for some of the machine

00:04:17.650 --> 00:04:22.369
ethics discussion, which is fascinating but
I go back to moral agency.

00:04:22.369 --> 00:04:30.509
Peter: So, if a machine had these rules encoded
in its control computer, would that robot

00:04:30.509 --> 00:04:31.559
have moral agency?

00:04:31.559 --> 00:04:33.800
Doug: If it could make that decision, yes.

00:04:33.800 --> 00:04:34.449
Peter: Okay.

00:04:34.449 --> 00:04:43.150
Doug: Yes, good point. It would. And at that
point, then we have ethical or unethical behavior.

00:04:43.150 --> 00:04:48.719
Peter: What robots today currently lack is
the ability to be able to make… first of

00:04:48.719 --> 00:04:52.330
all, sufficiently well understand what's going
on in the world but I think also, to understand

00:04:52.330 --> 00:04:57.180
the consequences of their actions, to predict
into the future. ‘If I did this, then it

00:04:57.180 --> 00:04:59.309
would cause harm or injury to a human being’.

00:04:59.309 --> 00:04:59.639
Doug: Yes.

00:04:59.639 --> 00:05:06.020
Peter: We're a long, long way off that in
robotics. We would need to have that kind

00:05:06.020 --> 00:05:10.039
of capability in order to implement something
like these laws of robotics.

00:05:10.039 --> 00:05:15.210
Doug: Absolutely, yes. What surprises me is
how fast we’ve come Peter, and you of all

00:05:15.210 --> 00:05:21.879
people would know that. The speed of what
you guys are doing in terms of robotics is staggering.

00:05:21.879 --> 00:05:27.080
Peter: We have come a long way but I think
we have got an awfully long way to go. I think

00:05:27.080 --> 00:05:32.710
just in terms of perceiving the state of world
we’ve got a lot of work to do there. In

00:05:32.710 --> 00:05:40.729
order to figure out the consequences of our
actions, robotically, yes I think that's a

00:05:40.729 --> 00:05:43.219
huge body of work yet to be done there.

00:05:43.219 --> 00:05:44.079
Doug: Yes. Yes, I agree.

