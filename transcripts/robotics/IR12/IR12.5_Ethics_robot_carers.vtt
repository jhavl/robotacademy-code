WEBVTT
Kind: captions
Language: en-GB

00:00:03.689 --> 00:00:08.740
Peter: One of my favorite robot movies in
recent times is called "Robot and Frank".

00:00:08.740 --> 00:00:13.200
And it's about the relationship between an
elderly man who is suffering the early stages

00:00:13.200 --> 00:00:19.680
of dementia - he used to be a cat burglar
- and the robot who looks after him. And Frank

00:00:19.680 --> 00:00:25.060
is a fairly curmudgeonly character, and his
son finds it pretty draining and exhausting

00:00:25.060 --> 00:00:31.660
to go and visit him and be abused. So he buys
his father a robot, because the robot will

00:00:31.660 --> 00:00:41.469
look after him. And it's kind of a less bother
for the son, Hunter, and he has effectively

00:00:41.469 --> 00:00:47.530
delegated the care of his father to a machine
because it's less painful for him to actually

00:00:47.530 --> 00:00:49.519
visit him, the father, himself.

00:00:49.519 --> 00:00:54.499
So you could perhaps argue that this father,
Frank, is going to be better looked after

00:00:54.499 --> 00:01:01.410
with the robot. Do you wonder whether Hunter
is doing the right thing here by buying his

00:01:01.410 --> 00:01:03.140
father a robot and walking away?

00:01:03.140 --> 00:01:14.750
Doug: If we look at deontology, deontology
is ‘deon’, duty. And key to deontology

00:01:14.750 --> 00:01:21.070
are the sets of duties that are required to
meet those criteria I laid out earlier. Autonomy,

00:01:21.070 --> 00:01:25.190
non-maleficence, right? So in this case we're
talking about autonomy and the right of the

00:01:25.190 --> 00:01:31.660
individual to live a full and considered life.
So the father in this case is being looked

00:01:31.660 --> 00:01:39.640
after by a machine, the son is abrogating
his duty to his father. And so one could say

00:01:39.640 --> 00:01:47.390
that that's unethical. Secondly, to what degree
though? I think that having a machine to look

00:01:47.390 --> 00:01:53.730
after somebody, as long as it fulfills the
autonomy criterion for the father, it's fine.

00:01:53.730 --> 00:01:58.720
As long as he is autonomous and can make decisions
outside of the robot - not a problem. The

00:01:58.720 --> 00:02:04.560
robot is serving his needs and is increasing
his autonomy. But at that point where, and

00:02:04.560 --> 00:02:10.750
this is really a paternalistic move to look
after an individual, right? But at the point

00:02:10.750 --> 00:02:15.390
that the individuals autonomy is impacted,
in other words they can't make decisions,

00:02:15.390 --> 00:02:20.010
or what decisions they made are affected by
the machine, then I'd argue it's unethical.

00:02:20.010 --> 00:02:24.740
And I'd also argue that the duty of care is
being abrogated by the son.

00:02:24.740 --> 00:02:29.250
Peter: Even if the machine would do a better
job of care than the son would himself?

00:02:29.250 --> 00:02:33.010
Doug: But this does not abrogate the individual
for the duty.

00:02:33.010 --> 00:02:34.010
Peter: Okay.

00:02:34.010 --> 00:02:35.010
Doug: You're with me?

00:02:35.010 --> 00:02:36.020
Peter: Yup.

00:02:36.020 --> 00:02:44.550
Doug: But correct, I agree with you on that.
But the duty of care to a parent is an ethical,

00:02:44.550 --> 00:02:51.410
it's a fundamental ethical sort of quandary
we all have to face as our parents get older.

00:02:51.410 --> 00:02:52.410
Peter: Absolutely.

00:02:52.410 --> 00:02:57.490
Doug: But ethics and deontology would argue
that you have this duty and you fulfill it.

00:02:57.490 --> 00:03:02.521
Peter: So what about the... let's go down
a generation instead of up a generation. We

00:03:02.521 --> 00:03:08.470
have a lot of discussion in this country about
the cost of preschool care, childcare, and

00:03:08.470 --> 00:03:14.740
that we can't get enough people to do that
work. So what does ethics say about having

00:03:14.740 --> 00:03:22.670
machines, robots looking after kindergarten
age children or younger? They're well fed.

00:03:22.670 --> 00:03:29.459
They're looked after. Their nappies and diapers
are changed. Maybe they’re played with,

00:03:29.459 --> 00:03:32.720
very enthusiastically, perhaps more than by
a human.

00:03:32.720 --> 00:03:38.400
Doug: Yeah, I'm with you. But I think that,
again we use the same argument in terms of

00:03:38.400 --> 00:03:47.410
duty, and that's the duty of the parent. The
filial duty, and what that entails. So again,

00:03:47.410 --> 00:03:59.060
if a robot supplements that filial duty, good.
But if it replaces it - not good. So it becomes

00:03:59.060 --> 00:04:04.660
a grey area that you have to negotiate, because
if a robot at the end of the day makes our

00:04:04.660 --> 00:04:09.810
children safer – well I'm for that. You
know, if it doesn't let them get onto the

00:04:09.810 --> 00:04:14.700
street and that sort of thing, I'd argue that's
great. But a robot can't replace the duty

00:04:14.700 --> 00:04:21.880
of care of the parent. It can't do it. At
least not now. But there's still that fundamental

00:04:21.880 --> 00:04:23.690
duty the individual has to their children.

00:04:23.690 --> 00:04:25.350
Peter: And to their parents.

00:04:25.350 --> 00:04:29.230
Doug: And to their parents, yeah. And you
can't escape that.

00:04:29.230 --> 00:04:34.400
Peter: So where is that written down, the
duty to your children and your parents?

00:04:34.400 --> 00:04:36.479
Doug: Oh, it's around.

