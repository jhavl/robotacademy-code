WEBVTT
Kind: captions
Language: en-GB

00:00:03.659 --> 00:00:08.270
Peter: Robotic cars are coming. We’ve seen
increased levels of automation in recent times

00:00:08.270 --> 00:00:15.500
in regular passenger cars. We’ve seen systems
like backing alarms, automatic parking and

00:00:15.500 --> 00:00:21.430
so on, adaptive cruise control. Google particularly,
recently, have got a really high profile in

00:00:21.430 --> 00:00:26.949
this space, but I know that most of the major
auto manufacturers have got projects looking

00:00:26.949 --> 00:00:32.619
at developing fully self-driving cars and
the number that gets bandied around a lot

00:00:32.619 --> 00:00:38.399
is that this technology is going to be on
the roads by the year 2020.

00:00:38.399 --> 00:00:42.660
Lots of people worry about the legal issues
around this. That if there is a self-driving

00:00:42.660 --> 00:00:48.270
car and if it causes an accident, it kills
somebody, whose fault is that? But you’ve

00:00:48.270 --> 00:00:53.250
got to I guess balance that against the fact
that human beings are terrible drivers. Every

00:00:53.250 --> 00:00:56.980
year, a million people are killed, and maybe
a hundred million people are injured on the

00:00:56.980 --> 00:01:03.559
road, by human drivers. So if we had robot
cars, and they perhaps do cause some injuries,

00:01:03.559 --> 00:01:08.940
but perhaps 100th of the number of injuries
we have now, what does ethics have to say

00:01:08.940 --> 00:01:14.979
about that? Is that a good thing or a bad
thing that robot cars would be accidentally

00:01:14.979 --> 00:01:18.970
killing people? Not as many, but perhaps still
some?

00:01:18.970 --> 00:01:27.300
Doug: Being over 50, I always look to the
past to answer the future and I’m thinking

00:01:27.300 --> 00:01:32.140
all right, let’s think of the design of
cars in the past. So you and I were raised

00:01:32.140 --> 00:01:36.239
at the time when there were no seatbelts.
When we were kids, we all jump in the back

00:01:36.239 --> 00:01:45.069
seat, there were five of us and we’re back
there like hamsters or rabbits. We had a 1964

00:01:45.069 --> 00:01:51.649
Chrysler Saratoga. The cars were designed
not for safety at all.

00:01:51.649 --> 00:01:53.640
They were designed to look good…

00:01:53.649 --> 00:01:55.910
Peter: And they did at that time.

00:01:55.910 --> 00:02:01.170
Doug: Yeah, and they were wonderful. And they’re
designed to go fast. We had a 383 V8 and it

00:02:01.170 --> 00:02:10.270
just flew. The mindset in those days was not
around efficiency because fuel consumption

00:02:10.270 --> 00:02:17.050
wasn’t an issue because the oil was cheap.
Safety- cars weren’t designed for safety.

00:02:17.050 --> 00:02:23.120
You got into an accident in those days, you
were scrambled inside the car because it wasn’t

00:02:23.120 --> 00:02:28.930
design friendly. Mercedes-Benz, Volvo, you
know the story. Automobile started to design

00:02:28.930 --> 00:02:34.700
around safety into the later ‘60s and into
the ‘70s. It became a major issue. Engines

00:02:34.700 --> 00:02:37.760
were designed to drop-down, they had seatbelts…

00:02:37.760 --> 00:02:39.080
Peter: Airbags…

00:02:39.080 --> 00:02:50.010
Doug: Airbags came along and then the brakes.
It all changed as safety became a criteria.

00:02:50.010 --> 00:02:54.130
Automobile deaths are the biggest killer in
most first world countries. They are a big

00:02:54.130 --> 00:02:56.680
killer, there’s no doubt about it, in terms
of violent death.

00:02:56.680 --> 00:02:57.520
Peter: Yeah.

00:02:57.520 --> 00:03:06.680
Doug: So the argument would be that, in fact,
ethically are automobiles being manufactured

00:03:06.680 --> 00:03:14.390
even today under the strictest sense of safety
to preserve human life? Because ethics would

00:03:14.390 --> 00:03:19.420
argue that they should be, that you should
be designing this car, so that it’s the

00:03:19.420 --> 00:03:26.420
safest car possible within sort of the economic
constraints. So if you argue from the past

00:03:26.420 --> 00:03:34.819
to the future and look at this construct of
robotics making less death, making the road

00:03:34.819 --> 00:03:41.540
safer, I would argue that it’s completely
ethically founded in terms of using robotics.

00:03:41.540 --> 00:03:43.970
I have no problem with that at all.

00:03:43.970 --> 00:03:46.610
Peter: So in a greater good sense, it’s
fantastic.

00:03:46.610 --> 00:03:49.880
Doug: In a teleological sense, absolutely,
yeah.

00:03:49.880 --> 00:03:56.280
Peter: But for the individual whose partner
was killed by a robotic vehicle, they’re

00:03:56.280 --> 00:03:59.030
not going to be happy. But I guess they’re
not going to be happy if they’re killed

00:03:59.030 --> 00:04:00.959
by a human-driven vehicle either.

00:04:00.959 --> 00:04:08.790
Doug: Yeah, so again, what’s the difference?
You’re right. It becomes a case then of

00:04:08.790 --> 00:04:17.720
law, who gets sued and how. Does it fall back
onto the car manufacturer to be sued in that

00:04:17.720 --> 00:04:24.280
case rather than the fallibility of the individual who lose their license or get a jail term for causing that?

00:04:24.280 --> 00:04:28.400
Peter: So then it becomes a legal issue and
no longer a moral issue?

00:04:28.400 --> 00:04:29.440
Doug: That’s my belief, yes.

00:04:29.440 --> 00:04:34.310
Peter: Okay, so the higher or the moral consideration
here is that we could dramatically reduce

00:04:34.310 --> 00:04:38.690
the number of people killed on the roads and
that’s a good thing.

00:04:38.690 --> 00:04:39.720
Doug: Absolutely, Peter. I believe that. Yeah.

